{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classifiers:\n",
    "#### This notebook contains various classifiers that can be used on a provided text dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import sklearn\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enter the name of the dataset you want to test into the file_in variable:\n",
    "##### The file_out dataset is the name of the dataset that will stored the number converted version of the text data. Set the name of file_out to the name of the converted dataset if you already have a dataset that has numeric representations of the dataset already instead of words. Otherwise, file_out will be the output file name that is the converted data of the file_in dataset that gets read in for classification for the tensorflow model. Also, if you already have a converted file, make sure that re_read is set to 0 so that it does not get overwritten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Enter filenames below:\n",
    "file_in = \"suggestions_trialA.csv\"\n",
    "file_out = \"trial_taskA_converted.csv\"\n",
    "### Enter column names for data and labels below:\n",
    "data_col = \"comments\"\n",
    "label_col = \"is_prompt_exists\"\n",
    "### Set re_read to 0 if you have a converted text file, 1 if it needs to be created\n",
    "re_read = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suggestions_trialA.csv\n",
      "id                  object\n",
      "comments            object\n",
      "is_prompt_exists     int64\n",
      "dtype: object\n",
      "Sample size: 2500\n",
      "      id                                           comments  is_prompt_exists\n",
      "0  663_3  Please enable removing language code from the ...                 1\n",
      "1  663_4  Note: in your .csproj file, there is a Support...                 0\n",
      "2  664_1  Wich means the new version not fully replaced ...                 0\n",
      "3  664_2  Some of my users will still receive the old xa...                 0\n",
      "4  664_3  The store randomly gives the old xap or the ne...                 0\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(file_in, engine = 'python');\n",
    "print(file_in)\n",
    "print(df.dtypes)\n",
    "print(\"Sample size:\", len(df))\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[data_col]\n",
    "Y = df[label_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    " X, Y, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Multinomial Naive Bayes\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english')),\n",
    " ('tfidf', TfidfTransformer()),\n",
    " ('clf', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = text_clf.fit(X_train,Y_train) # X and Y switched?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.747\n"
     ]
    }
   ],
   "source": [
    "predicted = text_clf.predict(X_test)\n",
    "accuracy = np.mean(predicted == Y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add graphics here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Support Vector Machine\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer()),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf-svm', SGDClassifier(loss='hinge',\n",
    "                                                  penalty='l2',\n",
    "                                                  max_iter=5,\n",
    "                                                  random_state=42)), \n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = text_clf_svm.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.821\n"
     ]
    }
   ],
   "source": [
    "predicted_svm = text_clf_svm.predict(X_test)\n",
    "accuracy = np.mean(predicted_svm == Y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add graphics here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridsearch classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Gridsearch\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'tfidf__use_idf': (True, False),\n",
    "              'clf-svm__alpha': (1e-2, 1e-3),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Can use on previous svm or bayes classifier\n",
    "gs_clf = GridSearchCV(text_clf_svm, parameters, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(X_train,Y_train)\n",
    "predicted_gs = gs_clf.predict(X_test)\n",
    "accuracy = np.mean(predicted_gs == Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search best score: 0.836\n",
      "{'clf-svm__alpha': 0.001, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 1)}\n",
      "Accuracy: 0.814\n"
     ]
    }
   ],
   "source": [
    "print(\"Grid search best score:\", gs_clf.best_score_)\n",
    "print(gs_clf.best_params_)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add graphics here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow/Keras Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a numeric version of this text data set if not present (uncomment below if to do so). Also determines length of vocabulary to use for embedding layer size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the word mappings\n",
    "def bagofwords():\n",
    "    a = []\n",
    "    loop = 0\n",
    "    for loop in range(num_data):\n",
    "        comment = df[data_col][loop]\n",
    "        sentTok = nltk.sent_tokenize(comment)\n",
    "        for sentence in sentTok:\n",
    "            a.append(sentence)    \n",
    "    a = (' '.join(a)).lower()\n",
    "    text = nltk.word_tokenize(a)\n",
    "    newset = set()\n",
    "    for word in text:\n",
    "        newset.add(word)\n",
    "    # Fill in found words\n",
    "    word_to_ix = { w:(i+4) for i,w in enumerate(sorted(newset)) }\n",
    "    ix_to_word = { (i+4):w for i,w in enumerate(sorted(newset)) }\n",
    "    # Fill in reserved values\n",
    "    ix_to_word[0] = \"<PAD>\"\n",
    "    ix_to_word[1] = \"<START>\"\n",
    "    ix_to_word[2] = \"<UNK>\"\n",
    "    ix_to_word[3] = \"<UNUSED>\"\n",
    "    word_to_ix[\"<PAD>\"] = 0 # Used to equalize text length\n",
    "    word_to_ix[\"<START>\"] = 1\n",
    "    word_to_ix[\"<UNK>\"] = 2  # unknown value\n",
    "    word_to_ix[\"<UNUSED>\"] = 3\n",
    "    return word_to_ix, ix_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary: 5752\n"
     ]
    }
   ],
   "source": [
    "word_to_ix, ix_to_word = bagofwords()\n",
    "# print(ix_to_word[3853])\n",
    "# print(word_to_ix[\"please\"]) # Ensure word is in dictionary\n",
    "print(\"Size of vocabulary:\", len(ix_to_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### An update text will be printed in order to keep track of the conversion process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Convert original data to number representations\n",
    "if (re_read == 1):\n",
    "    num_data = len(df)\n",
    "    # num_data = 5\n",
    "    loop = 0\n",
    "    converted_text_data = []\n",
    "    print(\"Total to print:\", num_data)\n",
    "    for loop in range(num_data):\n",
    "        if (loop % 5000 == 0): # For larger datasets\n",
    "            print(\"Update:\", loop) # Check spot in converter\n",
    "        comment = df[data_col][loop]\n",
    "        if isinstance(comment, str) == False: # Skip missing text from dataset if present\n",
    "            continue\n",
    "        comment = comment.lower()\n",
    "        text = nltk.word_tokenize(comment)\n",
    "        length = len(text)\n",
    "        i = 0\n",
    "    #     print(loop, \":\", text)\n",
    "        for i in range(length):\n",
    "    #         print(i, ':', word_to_ix[text[i]])\n",
    "            try:\n",
    "                text[i] = word_to_ix[text[i]]\n",
    "            except KeyError:\n",
    "                text[i] = 2 # Unknown mapping\n",
    "    #         print(text[i]) \n",
    "    #     print(text,'\\n')\n",
    "    #     print('\\n')\n",
    "        text = [int(i) for i in text]\n",
    "    #     print(text,'\\n\\n')\n",
    "        converted_text_data.append(text)\n",
    "\n",
    "    #     df['comments'][loop] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (re_read == 1):\n",
    "    # print(converted_text_data[0])\n",
    "    with open(file_out, 'w', newline = '') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([data_col, label_col])\n",
    "        i = 0\n",
    "        for i in range(num_data):\n",
    "    #         rev = datfram['Review'][i]\n",
    "            rev = converted_text_data[i]\n",
    "            label = df[label_col][i]\n",
    "    #         pol = (datfram['min_politeness'][i] + datfram['max_politeness'][i]) / 2.0\n",
    "    #         pol = int(round(pol, 0))\n",
    "    #         form = (datfram['min_formality'][i] + datfram['max_formality'][i]) / 2.0\n",
    "    #         form = int(round(form, 0))\n",
    "            writer.writerow([rev, label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (re_read == 1):\n",
    "    print(len(df))\n",
    "    print(len(converted_text_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in the converted text to number dataset and run the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial_taskA_converted.csv\n",
      "comments            object\n",
      "is_prompt_exists     int64\n",
      "dtype: object\n",
      "Sample size: 2500\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(file_out, engine = 'python');\n",
    "print(file_out)\n",
    "print(df.dtypes)\n",
    "print(\"Sample size:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 2500\n"
     ]
    }
   ],
   "source": [
    "### Parses the integer list data from file into a proper integer list for the classifier\n",
    "full_data = []\n",
    "for dat in df[data_col]:\n",
    "    paragraph = []\n",
    "    sentence = []\n",
    "    for letter in dat:\n",
    "        if (letter == '[' or letter == ']' or letter == ' '):\n",
    "            continue\n",
    "        elif (letter == ','):\n",
    "            sentence = (''.join(sentence))\n",
    "            paragraph.append(sentence)\n",
    "    #         print(sentence)\n",
    "            sentence = []\n",
    "        else:\n",
    "            sentence.append(letter)\n",
    "#     paragraph = (' '.join(paragraph))\n",
    "    paragraph = [int(i) for i in paragraph]\n",
    "    full_data.append(paragraph)\n",
    "# print(paragraph, '\\n')\n",
    "# full_data = traindata\n",
    "print(\"Dataset length:\", len(full_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number train data: 2000\n",
      "Number test data: 500\n"
     ]
    }
   ],
   "source": [
    "### Initialize train and test data, train and test labels\n",
    "num_data = len(df)\n",
    "num_train = int(num_data * 0.8) # df.head this amount\n",
    "num_test = int(num_data - num_train) # df.tail this amount\n",
    "train_data = full_data[:num_train]\n",
    "test_data = full_data[num_train:]\n",
    "train_labels = df[label_col].head(num_train) # is_prompt_exists\n",
    "test_labels = df[label_col].tail(num_test)\n",
    "print(\"Number train data:\", len(train_data))\n",
    "print(\"Number test data:\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max text length: 515\n"
     ]
    }
   ],
   "source": [
    "### Find the maximum sentence length to use for padding training data\n",
    "maxlength = 0\n",
    "for i in range(len(full_data)):\n",
    "    maxlength = max(len(full_data[i]), maxlength)\n",
    "print(\"Max text length:\", maxlength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pad the words to standardized text array length\n",
    "train_data = keras.preprocessing.sequence.pad_sequences(train_data,\n",
    "                                                       value=word_to_ix[\"<PAD>\"],\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=maxlength)\n",
    "\n",
    "test_data = keras.preprocessing.sequence.pad_sequences(test_data,\n",
    "                                                      value=word_to_ix[\"<PAD>\"],\n",
    "                                                      padding='post',\n",
    "                                                      maxlen=maxlength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data length: 1600\n",
      "Validation data length: 400\n"
     ]
    }
   ],
   "source": [
    "#### Initiate validation sets\n",
    "val_size = int(num_train * 0.8) # Set apart 20% of train data for validation\n",
    "\n",
    "x_val = train_data[:num_train-val_size]\n",
    "partial_x_train = train_data[num_train-val_size:]\n",
    "\n",
    "y_val = train_labels[:num_train-val_size]\n",
    "partial_y_train = train_labels[num_train-val_size:]\n",
    "\n",
    "print(\"Training data length:\", len(partial_x_train))\n",
    "print(\"Validation data length:\",len(x_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_16 (Embedding)     (None, None, 16)          92032     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_16  (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 92,321\n",
      "Trainable params: 92,321\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### Create the model\n",
    "import operator # Finding vocabulary length, max dictionary index + 1\n",
    "vocab_size = max(ix_to_word.items(), key=operator.itemgetter(1))[0] + 1\n",
    "\n",
    "model = keras.Sequential() \n",
    "model.add(keras.layers.Embedding(vocab_size, 16)) # original right parameter is 16\n",
    "model.add(keras.layers.GlobalAveragePooling1D())\n",
    "model.add(keras.layers.Dense(16, activation=tf.nn.relu)) # original left parameter  is 16\n",
    "model.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Configured model with optimizer and loss function\n",
    "model.compile(optimizer = tf.train.AdamOptimizer(),\n",
    "             loss = 'binary_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent_set = []\n",
    "# for i in range(len(full_data)):\n",
    "#     sentence = full_data[i]\n",
    "#     for word in sentence:\n",
    "#         sent_set.append(word)\n",
    "\n",
    "# # wor = (' '.join(sent_set[0]))\n",
    "# print(len(sent_set))\n",
    "# new_set = set(sent_set)\n",
    "# print(len(new_set),'\\n')\n",
    "# #5692\n",
    "# import operator\n",
    "# print(max(ix_to_word.items(), key=operator.itemgetter(1))[0])\n",
    "# for word in new_set:\n",
    "#     print(word)\n",
    "# print(word_to_ix[5741]) ## 5741\n",
    "# print(word_to_ix) ## 5751"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/40\n",
      "1600/1600 [==============================] - 1s 542us/step - loss: 0.6911 - acc: 0.5994 - val_loss: 0.6857 - val_acc: 0.8050\n",
      "Epoch 2/40\n",
      "1600/1600 [==============================] - 0s 44us/step - loss: 0.6838 - acc: 0.7863 - val_loss: 0.6786 - val_acc: 0.8050\n",
      "Epoch 3/40\n",
      "1600/1600 [==============================] - 0s 44us/step - loss: 0.6766 - acc: 0.7863 - val_loss: 0.6702 - val_acc: 0.8050\n",
      "Epoch 4/40\n",
      "1600/1600 [==============================] - 0s 42us/step - loss: 0.6684 - acc: 0.7863 - val_loss: 0.6611 - val_acc: 0.8050\n",
      "Epoch 5/40\n",
      "1600/1600 [==============================] - 0s 39us/step - loss: 0.6596 - acc: 0.7863 - val_loss: 0.6511 - val_acc: 0.8050\n",
      "Epoch 6/40\n",
      "1600/1600 [==============================] - 0s 37us/step - loss: 0.6500 - acc: 0.7863 - val_loss: 0.6400 - val_acc: 0.8050\n",
      "Epoch 7/40\n",
      "1600/1600 [==============================] - 0s 39us/step - loss: 0.6390 - acc: 0.7863 - val_loss: 0.6267 - val_acc: 0.8050\n",
      "Epoch 8/40\n",
      "1600/1600 [==============================] - 0s 38us/step - loss: 0.6261 - acc: 0.7863 - val_loss: 0.6125 - val_acc: 0.8050\n",
      "Epoch 9/40\n",
      "1600/1600 [==============================] - 0s 36us/step - loss: 0.6127 - acc: 0.7863 - val_loss: 0.5976 - val_acc: 0.8050\n",
      "Epoch 10/40\n",
      "1600/1600 [==============================] - 0s 37us/step - loss: 0.5987 - acc: 0.7863 - val_loss: 0.5809 - val_acc: 0.8050\n",
      "Epoch 11/40\n",
      "1600/1600 [==============================] - 0s 37us/step - loss: 0.5830 - acc: 0.7863 - val_loss: 0.5616 - val_acc: 0.8050\n",
      "Epoch 12/40\n",
      "1600/1600 [==============================] - 0s 37us/step - loss: 0.5648 - acc: 0.7863 - val_loss: 0.5417 - val_acc: 0.8050\n",
      "Epoch 13/40\n",
      "1600/1600 [==============================] - 0s 40us/step - loss: 0.5484 - acc: 0.7863 - val_loss: 0.5250 - val_acc: 0.8050\n",
      "Epoch 14/40\n",
      "1600/1600 [==============================] - 0s 46us/step - loss: 0.5354 - acc: 0.7863 - val_loss: 0.5127 - val_acc: 0.8050\n",
      "Epoch 15/40\n",
      "1600/1600 [==============================] - 0s 41us/step - loss: 0.5268 - acc: 0.7863 - val_loss: 0.5041 - val_acc: 0.8050\n",
      "Epoch 16/40\n",
      "1600/1600 [==============================] - 0s 41us/step - loss: 0.5213 - acc: 0.7863 - val_loss: 0.4987 - val_acc: 0.8050\n",
      "Epoch 17/40\n",
      "1600/1600 [==============================] - 0s 46us/step - loss: 0.5185 - acc: 0.7863 - val_loss: 0.4955 - val_acc: 0.8050\n",
      "Epoch 18/40\n",
      "1600/1600 [==============================] - 0s 45us/step - loss: 0.5173 - acc: 0.7863 - val_loss: 0.4938 - val_acc: 0.8050\n",
      "Epoch 19/40\n",
      "1600/1600 [==============================] - 0s 45us/step - loss: 0.5167 - acc: 0.7863 - val_loss: 0.4931 - val_acc: 0.8050\n",
      "Epoch 20/40\n",
      "1600/1600 [==============================] - 0s 45us/step - loss: 0.5166 - acc: 0.7863 - val_loss: 0.4927 - val_acc: 0.8050\n",
      "Epoch 21/40\n",
      "1600/1600 [==============================] - 0s 42us/step - loss: 0.5166 - acc: 0.7863 - val_loss: 0.4925 - val_acc: 0.8050\n",
      "Epoch 22/40\n",
      "1600/1600 [==============================] - 0s 43us/step - loss: 0.5164 - acc: 0.7863 - val_loss: 0.4924 - val_acc: 0.8050\n",
      "Epoch 23/40\n",
      "1600/1600 [==============================] - 0s 40us/step - loss: 0.5162 - acc: 0.7863 - val_loss: 0.4925 - val_acc: 0.8050\n",
      "Epoch 24/40\n",
      "1600/1600 [==============================] - 0s 45us/step - loss: 0.5159 - acc: 0.7863 - val_loss: 0.4927 - val_acc: 0.8050\n",
      "Epoch 25/40\n",
      "1600/1600 [==============================] - 0s 46us/step - loss: 0.5157 - acc: 0.7863 - val_loss: 0.4929 - val_acc: 0.8050\n",
      "Epoch 26/40\n",
      "1600/1600 [==============================] - 0s 44us/step - loss: 0.5154 - acc: 0.7863 - val_loss: 0.4927 - val_acc: 0.8050\n",
      "Epoch 27/40\n",
      "1600/1600 [==============================] - 0s 45us/step - loss: 0.5152 - acc: 0.7863 - val_loss: 0.4926 - val_acc: 0.8050\n",
      "Epoch 28/40\n",
      "1600/1600 [==============================] - 0s 40us/step - loss: 0.5150 - acc: 0.7863 - val_loss: 0.4928 - val_acc: 0.8050\n",
      "Epoch 29/40\n",
      "1600/1600 [==============================] - 0s 38us/step - loss: 0.5148 - acc: 0.7863 - val_loss: 0.4926 - val_acc: 0.8050\n",
      "Epoch 30/40\n",
      "1600/1600 [==============================] - 0s 40us/step - loss: 0.5146 - acc: 0.7863 - val_loss: 0.4924 - val_acc: 0.8050\n",
      "Epoch 31/40\n",
      "1600/1600 [==============================] - 0s 41us/step - loss: 0.5144 - acc: 0.7863 - val_loss: 0.4923 - val_acc: 0.8050\n",
      "Epoch 32/40\n",
      "1600/1600 [==============================] - 0s 46us/step - loss: 0.5142 - acc: 0.7863 - val_loss: 0.4923 - val_acc: 0.8050\n",
      "Epoch 33/40\n",
      "1600/1600 [==============================] - 0s 44us/step - loss: 0.5141 - acc: 0.7863 - val_loss: 0.4922 - val_acc: 0.8050\n",
      "Epoch 34/40\n",
      "1600/1600 [==============================] - 0s 43us/step - loss: 0.5138 - acc: 0.7863 - val_loss: 0.4922 - val_acc: 0.8050\n",
      "Epoch 35/40\n",
      "1600/1600 [==============================] - 0s 42us/step - loss: 0.5137 - acc: 0.7863 - val_loss: 0.4921 - val_acc: 0.8050\n",
      "Epoch 36/40\n",
      "1600/1600 [==============================] - 0s 42us/step - loss: 0.5135 - acc: 0.7863 - val_loss: 0.4924 - val_acc: 0.8050\n",
      "Epoch 37/40\n",
      "1600/1600 [==============================] - 0s 41us/step - loss: 0.5133 - acc: 0.7863 - val_loss: 0.4923 - val_acc: 0.8050\n",
      "Epoch 38/40\n",
      "1600/1600 [==============================] - 0s 39us/step - loss: 0.5131 - acc: 0.7863 - val_loss: 0.4921 - val_acc: 0.8050\n",
      "Epoch 39/40\n",
      "1600/1600 [==============================] - 0s 47us/step - loss: 0.5128 - acc: 0.7863 - val_loss: 0.4918 - val_acc: 0.8050\n",
      "Epoch 40/40\n",
      "1600/1600 [==============================] - 0s 43us/step - loss: 0.5126 - acc: 0.7863 - val_loss: 0.4916 - val_acc: 0.8050\n"
     ]
    }
   ],
   "source": [
    "### Train the model\n",
    "history = model.fit(partial_x_train,\n",
    "                   partial_y_train,\n",
    "                   epochs=40,\n",
    "                   batch_size=256,\n",
    "                   validation_data=(x_val, y_val),\n",
    "                   verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 0s 35us/step\n",
      "[0.6662123136520386, 0.6700000009536743]\n"
     ]
    }
   ],
   "source": [
    "### Evaluate the model\n",
    "results = model.evaluate(test_data, test_labels)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 734,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########### Graph the data\n",
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
