{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citation: Keras Model Code from [tensorflow.org](https://www.tensorflow.org/tutorials/keras/basic_text_classification) text classification\n",
    "Testing out data preprocessing and mdoels for text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7272\n",
      "is_prompt_exists     int64\n",
      "comments            object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "########## Read in the datafile\n",
    "df = pd.read_csv(\"suggestions_data_balanced.csv\", engine = 'python');\n",
    "df.fillna(-1, inplace=True)\n",
    "num_data = len(df)\n",
    "print(num_data)\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Shuffle the data for randomization purposes\n",
    "# df = sklearn.utils.shuffle(df)\n",
    "df = df.iloc[np.random.permutation(len(df))]\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: The authors need to be more explicit about the changes they will make. It is good that the team communicated that they will update the wiki to reflect the current DB schema, however instead of stating the potential changes, the structure of the entire wiki was instead presented. I think once the team dives a bit deeper into the project, they will be able to provide the names of the DB tables that were changed or removed. \n",
      "Label: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Data:\", df['comments'][0], \"\\nLabel:\", df['is_prompt_exists'][0]) # Check randomization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### Create \n",
    "def bagofwords():\n",
    "#     sentence = []\n",
    "#     sentence.append(\"Hello there my name is name is blank.\")\n",
    "#     sentence.append(\"This is another sample name there phrase.\")\n",
    "#     text = nltk.word_tokenize(sentence[0].lower())\n",
    "#     words = list(set(text))\n",
    "#     text = nltk.word_tokenize(sentence[1].lower())\n",
    "#     words.append(list(set(text)))\n",
    "#     print(words)\n",
    "    length = 0\n",
    "    a = []\n",
    "    for loop in range(num_data):\n",
    "        comment = df['comments'][loop]\n",
    "        sentTok = nltk.sent_tokenize(comment)\n",
    "        length = length + len(sentTok)\n",
    "        for sentence in sentTok:\n",
    "    #         a.append(\"Hello there my name is name is blank.\")\n",
    "    #         a.append(\"This is another sample name there phrase.\")\n",
    "            a.append(sentence)\n",
    "    \n",
    "    \n",
    "    a = (' '.join(a)).lower()\n",
    "#     print(a)\n",
    "#     print('\\n')\n",
    "    text = nltk.word_tokenize(a)\n",
    "#     print(text)\n",
    "    newset = set()\n",
    "    for word in text:\n",
    "#         print(word)\n",
    "        newset.add(word)\n",
    "#     print(newset)\n",
    "#     print('\\n\\n')\n",
    "    # Fill in found words\n",
    "    word_to_ix = { w:(i+4) for i,w in enumerate(sorted(newset)) }\n",
    "    ix_to_word = { (i+4):w for i,w in enumerate(sorted(newset)) }\n",
    "    # Fill in reserved values\n",
    "    ix_to_word[0] = \"<PAD>\"\n",
    "    ix_to_word[1] = \"<START>\"\n",
    "    ix_to_word[2] = \"<UNK>\"\n",
    "    ix_to_word[3] = \"<UNUSED>\"\n",
    "    word_to_ix[\"<PAD>\"] = 0 # Used to equalize text length\n",
    "    word_to_ix[\"<START>\"] = 1\n",
    "    word_to_ix[\"<UNK>\"] = 2  # unknown value\n",
    "    word_to_ix[\"<UNUSED>\"] = 3\n",
    "    return word_to_ix, ix_to_word\n",
    "# print(\"\\nLENGTH:\", length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix, ix_to_word = bagofwords()\n",
    "# print(ix_to_word)\n",
    "# print('\\n')\n",
    "# print(word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<UNUSED>\n",
      "4591\n",
      "5392\n",
      "5392\n"
     ]
    }
   ],
   "source": [
    "print(ix_to_word[3])\n",
    "print(word_to_ix[\"straightforward\"]) # Ensure word is in dictionary\n",
    "print(len(ix_to_word))\n",
    "print(len(word_to_ix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gabrielfinnrzingle/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update: 500\n",
      "Update: 1000\n",
      "Update: 1500\n",
      "Update: 2000\n",
      "Update: 2500\n",
      "Update: 3000\n",
      "Update: 3500\n",
      "Update: 4000\n",
      "Update: 4500\n",
      "Update: 5000\n",
      "Update: 5500\n",
      "Update: 6000\n",
      "Update: 6500\n",
      "Update: 7000\n"
     ]
    }
   ],
   "source": [
    "###################### Convert original data to number representations\n",
    "loop = 0\n",
    "for loop in range(num_data):\n",
    "    if (loop % 500 == 0):\n",
    "        print(\"Update:\", loop) # Check spot in converter\n",
    "    comment = df['comments'][loop]\n",
    "    comment = comment.lower()\n",
    "    text = nltk.word_tokenize(comment)\n",
    "    length = len(text)\n",
    "    for i in range(length):\n",
    "        text[i] = word_to_ix[text[i]]\n",
    "#         print(text[i]) \n",
    "#     print(text)\n",
    "    df['comments'][loop] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the changes mentioned are pretty straightforward , more information on how their plan to make those changes are should be provided in the document . \n",
      "\n",
      "the images and visual aids explain the flow very well . there were no uml diagrams in the document though . \n",
      "\n",
      "although , the writeup explains the functionality very well , they have n't mentioned which design pattern they have used an why ? . \n",
      "\n",
      "no new tests have been added \n",
      "\n",
      "changes are very good , the author has proposed design patterns they will be used in the implementation , along with that method to calculate score has been explained elaborately \n",
      "\n",
      "the design appears to be sound , but i think more details are needed . \n",
      "\n",
      "1. the plan is sound and seems to be clearly explained bullet points \n",
      "\n",
      "the principles used to solving the problem are sound \n",
      "\n",
      "yes , the code is well written and follows the conventions of ruby design principles . \n",
      "\n",
      "code written following the coding standards \n",
      "\n"
     ]
    }
   ],
   "source": [
    "##################### Convert words back to check conversion \n",
    "# for i in range(num_data):\n",
    "#     comment = df['comments'][i]\n",
    "#     length = len(comment)\n",
    "#     count = 0;\n",
    "#     text = []\n",
    "#     for num in comment:\n",
    "#         text.append(ix_to_word[num])\n",
    "#         count += 1\n",
    "#     text = (' '.join(text))\n",
    "#     print(text,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Initialize training data\n",
    "num_train = int(num_data * 0.8) # df.head this amount\n",
    "num_test = int(num_data - num_train) # df.tail this amount\n",
    "# print(num_train)\n",
    "# print(num_test)\n",
    "# print(len(df), \"  \", num_train+num_test)\n",
    "train_data = df['comments'].head(num_train)\n",
    "# for i in range(8):\n",
    "#     print(train_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Initialize testing data **(requires full dataset conversion for tail() to work)**\n",
    "# print(df['comments'].tail(2))\n",
    "test_data = df['comments'].tail(num_test)\n",
    "# test_data = df['comments'].head(4) #### Remove later (PLACEHOLDER STATEMENT)\n",
    "# for i in range(2):\n",
    "#     print(test_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Function enabling conversion of indexed number sentence into word sentence\n",
    "def decode_review(text):\n",
    "#     return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
    "    return ' '.join([ix_to_word[i] for i in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "they very thoroughly tested the job applications model and controller . there were tests for the admin user but i could n't replicate them on the actual website so either the tests did n't pass or the user given in the readme was n't an admin .\n"
     ]
    }
   ],
   "source": [
    "print(decode_review(train_data[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 343\n",
      "Test: 343\n"
     ]
    }
   ],
   "source": [
    "############# Find the maximum sentence length to use for padding training data\n",
    "maxlength = 0\n",
    "for array in train_data:\n",
    "    maxlength = max(len(array),maxlength)\n",
    "print(\"Train:\", maxlength)\n",
    "for array in test_data:\n",
    "    maxlength = max(len(array),maxlength)\n",
    "print(\"Test:\", maxlength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "############# Initialize training and test labels\n",
    "train_labels = df['is_prompt_exists'].head(num_train)\n",
    "test_labels = df['is_prompt_exists'].tail(num_test)\n",
    "for i in range(10):\n",
    "    print(train_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Pad the words to equalize array length\n",
    "train_data = keras.preprocessing.sequence.pad_sequences(train_data,\n",
    "                                                       value=word_to_ix[\"<PAD>\"],\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=maxlength)\n",
    "\n",
    "test_data = keras.preprocessing.sequence.pad_sequences(test_data,\n",
    "                                                      value=word_to_ix[\"<PAD>\"],\n",
    "                                                      padding='post',\n",
    "                                                      maxlen=maxlength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(343, 343)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data[0]), len(train_data[1]) # Check the new length of some train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4853  760 3247 4916  827 3188 2008  405 4853 1026 4866 5306 2993  110\n",
      " 2738 2729 2299 4851 4853 4781 1170 4851 4866 5306 5074 4853 5302 4916\n",
      " 3937 4853 1445 1474 4213   85 2464 2653 3347 4565 4853 3634 1026   85\n",
      " 4853 4609 3347 4853 1877 5302 5241 2653 3670  110 2477 4870 3364 4853\n",
      " 4781 1691  398  869 1510 2700 4853 3733   85 4866 5306  827  404 4916\n",
      " 3769 4853 3223 3347 4853 1474 4749 4851 5275 1025 3397 3981  110    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0]) # Check new padded number sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 16)          116352    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 116,641\n",
      "Trainable params: 116,641\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "############# Creates the model\n",
    "vocab_size = num_data # suggestions_data_balanced.csv dataset length becomes input shape: 7272\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, 16))\n",
    "model.add(keras.layers.GlobalAveragePooling1D())\n",
    "model.add(keras.layers.Dense(16, activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Configured model with optimizer and loss function\n",
    "model.compile(optimizer = tf.train.AdamOptimizer(),\n",
    "             loss = 'binary_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Initiate validation sets\n",
    "# Note -> vocab_size = len(df)\n",
    "val_size = int(num_train * 0.1) # Set apart 10% of train data for validation\n",
    "\n",
    "x_val = train_data[:num_train-val_size]\n",
    "partial_x_train = train_data[num_train-val_size:]\n",
    "\n",
    "y_val = train_labels[:num_train-val_size]\n",
    "partial_y_train = train_labels[num_train-val_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5817\n",
      "5236\n",
      "581\n"
     ]
    }
   ],
   "source": [
    "# print(x_val[0])\n",
    "# print(partial_x_train[0])\n",
    "print(len(train_data))\n",
    "print(len(x_val))\n",
    "print(len(partial_x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 581 samples, validate on 5236 samples\n",
      "Epoch 1/40\n",
      "581/581 [==============================] - 0s 105us/step - loss: 0.0981 - acc: 0.0017 - val_loss: 0.0248 - val_acc: 7.6394e-04\n",
      "Epoch 2/40\n",
      "581/581 [==============================] - 0s 100us/step - loss: 0.0905 - acc: 0.0017 - val_loss: 0.0157 - val_acc: 7.6394e-04\n",
      "Epoch 3/40\n",
      "581/581 [==============================] - 0s 98us/step - loss: 0.0823 - acc: 0.0017 - val_loss: 0.0067 - val_acc: 7.6394e-04\n",
      "Epoch 4/40\n",
      "581/581 [==============================] - 0s 99us/step - loss: 0.0742 - acc: 0.0017 - val_loss: -0.0028 - val_acc: 7.6394e-04\n",
      "Epoch 5/40\n",
      "581/581 [==============================] - 0s 99us/step - loss: 0.0668 - acc: 0.0017 - val_loss: -0.0123 - val_acc: 7.6394e-04\n",
      "Epoch 6/40\n",
      "581/581 [==============================] - 0s 99us/step - loss: 0.0585 - acc: 0.0017 - val_loss: -0.0215 - val_acc: 7.6394e-04\n",
      "Epoch 7/40\n",
      "581/581 [==============================] - 0s 100us/step - loss: 0.0512 - acc: 0.0017 - val_loss: -0.0303 - val_acc: 7.6394e-04\n",
      "Epoch 8/40\n",
      "581/581 [==============================] - 0s 100us/step - loss: 0.0438 - acc: 0.0017 - val_loss: -0.0382 - val_acc: 7.6394e-04\n",
      "Epoch 9/40\n",
      "581/581 [==============================] - 0s 99us/step - loss: 0.0360 - acc: 0.0017 - val_loss: -0.0457 - val_acc: 7.6394e-04\n",
      "Epoch 10/40\n",
      "581/581 [==============================] - 0s 103us/step - loss: 0.0295 - acc: 0.0017 - val_loss: -0.0537 - val_acc: 7.6394e-04\n",
      "Epoch 11/40\n",
      "581/581 [==============================] - 0s 101us/step - loss: 0.0224 - acc: 0.0017 - val_loss: -0.0617 - val_acc: 0.0013\n",
      "Epoch 12/40\n",
      "581/581 [==============================] - 0s 105us/step - loss: 0.0157 - acc: 0.0017 - val_loss: -0.0699 - val_acc: 0.0013\n",
      "Epoch 13/40\n",
      "581/581 [==============================] - 0s 104us/step - loss: 0.0085 - acc: 0.0017 - val_loss: -0.0781 - val_acc: 0.0013\n",
      "Epoch 14/40\n",
      "581/581 [==============================] - 0s 100us/step - loss: 0.0018 - acc: 0.0017 - val_loss: -0.0862 - val_acc: 0.0013\n",
      "Epoch 15/40\n",
      "581/581 [==============================] - 0s 99us/step - loss: -0.0053 - acc: 0.0017 - val_loss: -0.0945 - val_acc: 0.0013\n",
      "Epoch 16/40\n",
      "581/581 [==============================] - 0s 97us/step - loss: -0.0115 - acc: 0.0017 - val_loss: -0.1028 - val_acc: 0.0013\n",
      "Epoch 17/40\n",
      "581/581 [==============================] - 0s 99us/step - loss: -0.0193 - acc: 0.0017 - val_loss: -0.1108 - val_acc: 0.0013\n",
      "Epoch 18/40\n",
      "581/581 [==============================] - 0s 100us/step - loss: -0.0261 - acc: 0.0017 - val_loss: -0.1187 - val_acc: 0.0013\n",
      "Epoch 19/40\n",
      "581/581 [==============================] - 0s 99us/step - loss: -0.0335 - acc: 0.0034 - val_loss: -0.1263 - val_acc: 0.0013\n",
      "Epoch 20/40\n",
      "581/581 [==============================] - 0s 98us/step - loss: -0.0403 - acc: 0.0034 - val_loss: -0.1343 - val_acc: 0.0013\n",
      "Epoch 21/40\n",
      "581/581 [==============================] - 0s 101us/step - loss: -0.0464 - acc: 0.0034 - val_loss: -0.1424 - val_acc: 0.0013\n",
      "Epoch 22/40\n",
      "581/581 [==============================] - 0s 98us/step - loss: -0.0545 - acc: 0.0034 - val_loss: -0.1502 - val_acc: 0.0013\n",
      "Epoch 23/40\n",
      "581/581 [==============================] - 0s 100us/step - loss: -0.0620 - acc: 0.0034 - val_loss: -0.1586 - val_acc: 0.0013\n",
      "Epoch 24/40\n",
      "581/581 [==============================] - 0s 99us/step - loss: -0.0707 - acc: 0.0034 - val_loss: -0.1665 - val_acc: 0.0019\n",
      "Epoch 25/40\n",
      "581/581 [==============================] - 0s 102us/step - loss: -0.0776 - acc: 0.0034 - val_loss: -0.1739 - val_acc: 0.0021\n",
      "Epoch 26/40\n",
      "581/581 [==============================] - 0s 99us/step - loss: -0.0840 - acc: 0.0052 - val_loss: -0.1811 - val_acc: 0.0029\n",
      "Epoch 27/40\n",
      "581/581 [==============================] - 0s 100us/step - loss: -0.0905 - acc: 0.0052 - val_loss: -0.1880 - val_acc: 0.0029\n",
      "Epoch 28/40\n",
      "581/581 [==============================] - 0s 98us/step - loss: -0.0975 - acc: 0.0052 - val_loss: -0.1949 - val_acc: 0.0031\n",
      "Epoch 29/40\n",
      "581/581 [==============================] - 0s 100us/step - loss: -0.1038 - acc: 0.0052 - val_loss: -0.2021 - val_acc: 0.0031\n",
      "Epoch 30/40\n",
      "581/581 [==============================] - 0s 108us/step - loss: -0.1108 - acc: 0.0052 - val_loss: -0.2087 - val_acc: 0.0031\n",
      "Epoch 31/40\n",
      "581/581 [==============================] - 0s 107us/step - loss: -0.1175 - acc: 0.0052 - val_loss: -0.2151 - val_acc: 0.0034\n",
      "Epoch 32/40\n",
      "581/581 [==============================] - 0s 109us/step - loss: -0.1243 - acc: 0.0052 - val_loss: -0.2215 - val_acc: 0.0034\n",
      "Epoch 33/40\n",
      "581/581 [==============================] - 0s 109us/step - loss: -0.1312 - acc: 0.0052 - val_loss: -0.2283 - val_acc: 0.0034\n",
      "Epoch 34/40\n",
      "581/581 [==============================] - 0s 110us/step - loss: -0.1377 - acc: 0.0052 - val_loss: -0.2359 - val_acc: 0.0036\n",
      "Epoch 35/40\n",
      "581/581 [==============================] - 0s 108us/step - loss: -0.1448 - acc: 0.0052 - val_loss: -0.2431 - val_acc: 0.0052\n",
      "Epoch 36/40\n",
      "581/581 [==============================] - 0s 110us/step - loss: -0.1518 - acc: 0.0069 - val_loss: -0.2505 - val_acc: 0.0052\n",
      "Epoch 37/40\n",
      "581/581 [==============================] - 0s 126us/step - loss: -0.1588 - acc: 0.0069 - val_loss: -0.2581 - val_acc: 0.0055\n",
      "Epoch 38/40\n",
      "581/581 [==============================] - 0s 120us/step - loss: -0.1656 - acc: 0.0069 - val_loss: -0.2664 - val_acc: 0.0057\n",
      "Epoch 39/40\n",
      "581/581 [==============================] - 0s 119us/step - loss: -0.1726 - acc: 0.0069 - val_loss: -0.2751 - val_acc: 0.0057\n",
      "Epoch 40/40\n",
      "581/581 [==============================] - 0s 117us/step - loss: -0.1810 - acc: 0.0069 - val_loss: -0.2830 - val_acc: 0.0061\n"
     ]
    }
   ],
   "source": [
    "############ Trains the model\n",
    "history = model.fit(partial_x_train,\n",
    "                   partial_y_train,\n",
    "                   epochs=40,\n",
    "                   batch_size=512,\n",
    "                   validation_data=(x_val, y_val),\n",
    "                   verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1455/1455 [==============================] - 0s 21us/step\n",
      "[-0.39611632033312033, 0.008934707903780068]\n"
     ]
    }
   ],
   "source": [
    "########### Evaluate the model\n",
    "results = model.evaluate(test_data, test_labels)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Graph the data\n",
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
