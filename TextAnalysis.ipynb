{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figuring out data preprocessing for text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_prompt_exists     int64\n",
      "comments            object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "########## Read in the datafile\n",
    "df = pd.read_csv(\"suggestions_data_balanced.csv\", engine = 'python');\n",
    "df.fillna(-1, inplace=True)\n",
    "\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Declare constants\n",
    "num_data = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### Create \n",
    "def bagofwords():\n",
    "#     sentence = []\n",
    "#     sentence.append(\"Hello there my name is name is blank.\")\n",
    "#     sentence.append(\"This is another sample name there phrase.\")\n",
    "#     text = nltk.word_tokenize(sentence[0].lower())\n",
    "#     words = list(set(text))\n",
    "#     text = nltk.word_tokenize(sentence[1].lower())\n",
    "#     words.append(list(set(text)))\n",
    "#     print(words)\n",
    "    length = 0\n",
    "    a = []\n",
    "    for loop in range(num_data):\n",
    "        comment = df['comments'][loop]\n",
    "        sentTok = nltk.sent_tokenize(comment)\n",
    "        length = length + len(sentTok)\n",
    "        for sentence in sentTok:\n",
    "    #         a.append(\"Hello there my name is name is blank.\")\n",
    "    #         a.append(\"This is another sample name there phrase.\")\n",
    "            a.append(sentence)\n",
    "    \n",
    "    \n",
    "    a = (' '.join(a)).lower()\n",
    "#     print(a)\n",
    "#     print('\\n')\n",
    "    text = nltk.word_tokenize(a)\n",
    "#     print(text)\n",
    "    newset = set()\n",
    "    for word in text:\n",
    "#         print(word)\n",
    "        newset.add(word)\n",
    "#     print(newset)\n",
    "#     print('\\n\\n')\n",
    "\n",
    "    word_to_ix = { w:i for i,w in enumerate(sorted(newset)) }\n",
    "    ix_to_word = { i:w for i,w in enumerate(sorted(newset)) }\n",
    "    return word_to_ix, ix_to_word\n",
    "# print(\"\\nLENGTH:\", length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: ',', 1: '.', 2: '1.', 3: '?', 4: 'added', 5: 'aids', 6: 'along', 7: 'although', 8: 'an', 9: 'and', 10: 'appears', 11: 'are', 12: 'author', 13: 'be', 14: 'been', 15: 'bullet', 16: 'but', 17: 'calculate', 18: 'changes', 19: 'clearly', 20: 'code', 21: 'coding', 22: 'conventions', 23: 'design', 24: 'details', 25: 'diagrams', 26: 'document', 27: 'elaborately', 28: 'explain', 29: 'explained', 30: 'explains', 31: 'flow', 32: 'following', 33: 'follows', 34: 'functionality', 35: 'good', 36: 'has', 37: 'have', 38: 'how', 39: 'i', 40: 'images', 41: 'implementation', 42: 'in', 43: 'information', 44: 'is', 45: 'make', 46: 'mentioned', 47: 'method', 48: 'more', 49: \"n't\", 50: 'needed', 51: 'new', 52: 'no', 53: 'of', 54: 'on', 55: 'pattern', 56: 'patterns', 57: 'plan', 58: 'points', 59: 'pretty', 60: 'principles', 61: 'problem', 62: 'proposed', 63: 'provided', 64: 'ruby', 65: 'score', 66: 'seems', 67: 'should', 68: 'solving', 69: 'sound', 70: 'standards', 71: 'straightforward', 72: 'tests', 73: 'that', 74: 'the', 75: 'their', 76: 'there', 77: 'they', 78: 'think', 79: 'those', 80: 'though', 81: 'to', 82: 'uml', 83: 'used', 84: 'very', 85: 'visual', 86: 'well', 87: 'were', 88: 'which', 89: 'why', 90: 'will', 91: 'with', 92: 'writeup', 93: 'written', 94: 'yes'}\n",
      "\n",
      "\n",
      "{',': 0, '.': 1, '1.': 2, '?': 3, 'added': 4, 'aids': 5, 'along': 6, 'although': 7, 'an': 8, 'and': 9, 'appears': 10, 'are': 11, 'author': 12, 'be': 13, 'been': 14, 'bullet': 15, 'but': 16, 'calculate': 17, 'changes': 18, 'clearly': 19, 'code': 20, 'coding': 21, 'conventions': 22, 'design': 23, 'details': 24, 'diagrams': 25, 'document': 26, 'elaborately': 27, 'explain': 28, 'explained': 29, 'explains': 30, 'flow': 31, 'following': 32, 'follows': 33, 'functionality': 34, 'good': 35, 'has': 36, 'have': 37, 'how': 38, 'i': 39, 'images': 40, 'implementation': 41, 'in': 42, 'information': 43, 'is': 44, 'make': 45, 'mentioned': 46, 'method': 47, 'more': 48, \"n't\": 49, 'needed': 50, 'new': 51, 'no': 52, 'of': 53, 'on': 54, 'pattern': 55, 'patterns': 56, 'plan': 57, 'points': 58, 'pretty': 59, 'principles': 60, 'problem': 61, 'proposed': 62, 'provided': 63, 'ruby': 64, 'score': 65, 'seems': 66, 'should': 67, 'solving': 68, 'sound': 69, 'standards': 70, 'straightforward': 71, 'tests': 72, 'that': 73, 'the': 74, 'their': 75, 'there': 76, 'they': 77, 'think': 78, 'those': 79, 'though': 80, 'to': 81, 'uml': 82, 'used': 83, 'very': 84, 'visual': 85, 'well': 86, 'were': 87, 'which': 88, 'why': 89, 'will': 90, 'with': 91, 'writeup': 92, 'written': 93, 'yes': 94}\n"
     ]
    }
   ],
   "source": [
    "word_to_ix, ix_to_word = bagofwords()\n",
    "print(ix_to_word)\n",
    "print('\\n')\n",
    "print(word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aids\n",
      "71\n"
     ]
    }
   ],
   "source": [
    "print(ix_to_word[5]) #be\n",
    "print(word_to_ix[\"straightforward\"]) #24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gabrielfinnrzingle/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "###################### Convert original data to number representations\n",
    "loop = 0\n",
    "for loop in range(num_data):\n",
    "    comment = df['comments'][loop]\n",
    "    comment = comment.lower()\n",
    "    text = nltk.word_tokenize(comment)\n",
    "    length = len(text)\n",
    "    for i in range(length):\n",
    "        text[i] = word_to_ix[text[i]]\n",
    "#         print(text[i]) \n",
    "#     print(text)\n",
    "    df['comments'][loop] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the changes mentioned are pretty straightforward , more information on how their plan to make those changes are should be provided in the document .\n",
      "the images and visual aids explain the flow very well . there were no uml diagrams in the document though .\n",
      "although , the writeup explains the functionality very well , they have n't mentioned which design pattern they have used an why ? .\n",
      "no new tests have been added\n",
      "changes are very good , the author has proposed design patterns they will be used in the implementation , along with that method to calculate score has been explained elaborately\n",
      "the design appears to be sound , but i think more details are needed .\n",
      "1. the plan is sound and seems to be clearly explained bullet points\n",
      "the principles used to solving the problem are sound\n",
      "yes , the code is well written and follows the conventions of ruby design principles .\n",
      "code written following the coding standards\n"
     ]
    }
   ],
   "source": [
    "##################### Convert words back to check conversion \n",
    "for i in range(num_data):\n",
    "    comment = df['comments'][i]\n",
    "    length = len(comment)\n",
    "    count = 0;\n",
    "    text = []\n",
    "    for num in comment:\n",
    "        text.append(ix_to_word[num])\n",
    "        count += 1\n",
    "    text = (' '.join(text))\n",
    "    print(text,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[74, 18, 46, 11, 59, 71, 0, 48, 43, 54, 38, 75, 57, 81, 45, 79, 18, 11, 67, 13, 63, 42, 74, 26, 1]\n",
      "[74, 40, 9, 85, 5, 28, 74, 31, 84, 86, 1, 76, 87, 52, 82, 25, 42, 74, 26, 80, 1]\n",
      "[7, 0, 74, 92, 30, 74, 34, 84, 86, 0, 77, 37, 49, 46, 88, 23, 55, 77, 37, 83, 8, 89, 3, 1]\n",
      "[52, 51, 72, 37, 14, 4]\n",
      "[18, 11, 84, 35, 0, 74, 12, 36, 62, 23, 56, 77, 90, 13, 83, 42, 74, 41, 0, 6, 91, 73, 47, 81, 17, 65, 36, 14, 29, 27]\n",
      "[74, 23, 10, 81, 13, 69, 0, 16, 39, 78, 48, 24, 11, 50, 1]\n",
      "[2, 74, 57, 44, 69, 9, 66, 81, 13, 19, 29, 15, 58]\n",
      "[74, 60, 83, 81, 68, 74, 61, 11, 69]\n"
     ]
    }
   ],
   "source": [
    "############# Initialize training data\n",
    "train_data = df['comments'].head(8)\n",
    "for i in range(8):\n",
    "    print(train_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Initialize testing data (requires full dataset conversion for tail() to work)\n",
    "# print(df['comments'].tail(2))\n",
    "# test_data = df['comments'].tail(2)\n",
    "# for i in range(2):\n",
    "#     print(test_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
