{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citation: Keras Model Code from [tensorflow.org](https://www.tensorflow.org/tutorials/keras/basic_text_classification) text classification\n",
    "Testing out data preprocessing and mdoels for text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_prompt_exists     int64\n",
      "comments            object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "########## Read in the datafile\n",
    "df = pd.read_csv(\"suggestions_data_balanced.csv\", engine = 'python');\n",
    "df.fillna(-1, inplace=True)\n",
    "\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Declare constants\n",
    "num_data = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### Create \n",
    "def bagofwords():\n",
    "#     sentence = []\n",
    "#     sentence.append(\"Hello there my name is name is blank.\")\n",
    "#     sentence.append(\"This is another sample name there phrase.\")\n",
    "#     text = nltk.word_tokenize(sentence[0].lower())\n",
    "#     words = list(set(text))\n",
    "#     text = nltk.word_tokenize(sentence[1].lower())\n",
    "#     words.append(list(set(text)))\n",
    "#     print(words)\n",
    "    length = 0\n",
    "    a = []\n",
    "    for loop in range(num_data):\n",
    "        comment = df['comments'][loop]\n",
    "        sentTok = nltk.sent_tokenize(comment)\n",
    "        length = length + len(sentTok)\n",
    "        for sentence in sentTok:\n",
    "    #         a.append(\"Hello there my name is name is blank.\")\n",
    "    #         a.append(\"This is another sample name there phrase.\")\n",
    "            a.append(sentence)\n",
    "    \n",
    "    \n",
    "    a = (' '.join(a)).lower()\n",
    "#     print(a)\n",
    "#     print('\\n')\n",
    "    text = nltk.word_tokenize(a)\n",
    "#     print(text)\n",
    "    newset = set()\n",
    "    for word in text:\n",
    "#         print(word)\n",
    "        newset.add(word)\n",
    "#     print(newset)\n",
    "#     print('\\n\\n')\n",
    "    # Fill in found words\n",
    "    word_to_ix = { w:(i+4) for i,w in enumerate(sorted(newset)) }\n",
    "    ix_to_word = { (i+4):w for i,w in enumerate(sorted(newset)) }\n",
    "    # Fill in reserved values\n",
    "    ix_to_word[0] = \"<PAD>\"\n",
    "    ix_to_word[1] = \"<START>\"\n",
    "    ix_to_word[2] = \"<UNK>\"\n",
    "    ix_to_word[3] = \"<UNUSED>\"\n",
    "    word_to_ix[\"<PAD>\"] = 0 # Used to equalize text length\n",
    "    word_to_ix[\"<START>\"] = 1\n",
    "    word_to_ix[\"<UNK>\"] = 2  # unknown value\n",
    "    word_to_ix[\"<UNUSED>\"] = 3\n",
    "    return word_to_ix, ix_to_word\n",
    "# print(\"\\nLENGTH:\", length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{4: ',', 5: '.', 6: '1.', 7: '?', 8: 'added', 9: 'aids', 10: 'along', 11: 'although', 12: 'an', 13: 'and', 14: 'appears', 15: 'are', 16: 'author', 17: 'be', 18: 'been', 19: 'bullet', 20: 'but', 21: 'calculate', 22: 'changes', 23: 'clearly', 24: 'code', 25: 'coding', 26: 'conventions', 27: 'design', 28: 'details', 29: 'diagrams', 30: 'document', 31: 'elaborately', 32: 'explain', 33: 'explained', 34: 'explains', 35: 'flow', 36: 'following', 37: 'follows', 38: 'functionality', 39: 'good', 40: 'has', 41: 'have', 42: 'how', 43: 'i', 44: 'images', 45: 'implementation', 46: 'in', 47: 'information', 48: 'is', 49: 'make', 50: 'mentioned', 51: 'method', 52: 'more', 53: \"n't\", 54: 'needed', 55: 'new', 56: 'no', 57: 'of', 58: 'on', 59: 'pattern', 60: 'patterns', 61: 'plan', 62: 'points', 63: 'pretty', 64: 'principles', 65: 'problem', 66: 'proposed', 67: 'provided', 68: 'ruby', 69: 'score', 70: 'seems', 71: 'should', 72: 'solving', 73: 'sound', 74: 'standards', 75: 'straightforward', 76: 'tests', 77: 'that', 78: 'the', 79: 'their', 80: 'there', 81: 'they', 82: 'think', 83: 'those', 84: 'though', 85: 'to', 86: 'uml', 87: 'used', 88: 'very', 89: 'visual', 90: 'well', 91: 'were', 92: 'which', 93: 'why', 94: 'will', 95: 'with', 96: 'writeup', 97: 'written', 98: 'yes', 0: '<PAD>', 1: '<START>', 2: '<UNK>', 3: '<UNUSED>'}\n",
      "\n",
      "\n",
      "{',': 4, '.': 5, '1.': 6, '?': 7, 'added': 8, 'aids': 9, 'along': 10, 'although': 11, 'an': 12, 'and': 13, 'appears': 14, 'are': 15, 'author': 16, 'be': 17, 'been': 18, 'bullet': 19, 'but': 20, 'calculate': 21, 'changes': 22, 'clearly': 23, 'code': 24, 'coding': 25, 'conventions': 26, 'design': 27, 'details': 28, 'diagrams': 29, 'document': 30, 'elaborately': 31, 'explain': 32, 'explained': 33, 'explains': 34, 'flow': 35, 'following': 36, 'follows': 37, 'functionality': 38, 'good': 39, 'has': 40, 'have': 41, 'how': 42, 'i': 43, 'images': 44, 'implementation': 45, 'in': 46, 'information': 47, 'is': 48, 'make': 49, 'mentioned': 50, 'method': 51, 'more': 52, \"n't\": 53, 'needed': 54, 'new': 55, 'no': 56, 'of': 57, 'on': 58, 'pattern': 59, 'patterns': 60, 'plan': 61, 'points': 62, 'pretty': 63, 'principles': 64, 'problem': 65, 'proposed': 66, 'provided': 67, 'ruby': 68, 'score': 69, 'seems': 70, 'should': 71, 'solving': 72, 'sound': 73, 'standards': 74, 'straightforward': 75, 'tests': 76, 'that': 77, 'the': 78, 'their': 79, 'there': 80, 'they': 81, 'think': 82, 'those': 83, 'though': 84, 'to': 85, 'uml': 86, 'used': 87, 'very': 88, 'visual': 89, 'well': 90, 'were': 91, 'which': 92, 'why': 93, 'will': 94, 'with': 95, 'writeup': 96, 'written': 97, 'yes': 98, '<PAD>': 0, '<START>': 1, '<UNK>': 2, '<UNUSED>': 3}\n"
     ]
    }
   ],
   "source": [
    "word_to_ix, ix_to_word = bagofwords()\n",
    "print(ix_to_word)\n",
    "print('\\n')\n",
    "print(word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<UNUSED>\n",
      "75\n"
     ]
    }
   ],
   "source": [
    "print(ix_to_word[3])\n",
    "print(word_to_ix[\"straightforward\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gabrielfinnrzingle/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "###################### Convert original data to number representations\n",
    "loop = 0\n",
    "for loop in range(num_data):\n",
    "    comment = df['comments'][loop]\n",
    "    comment = comment.lower()\n",
    "    text = nltk.word_tokenize(comment)\n",
    "    length = len(text)\n",
    "    for i in range(length):\n",
    "        text[i] = word_to_ix[text[i]]\n",
    "#         print(text[i]) \n",
    "#     print(text)\n",
    "    df['comments'][loop] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the changes mentioned are pretty straightforward , more information on how their plan to make those changes are should be provided in the document . \n",
      "\n",
      "the images and visual aids explain the flow very well . there were no uml diagrams in the document though . \n",
      "\n",
      "although , the writeup explains the functionality very well , they have n't mentioned which design pattern they have used an why ? . \n",
      "\n",
      "no new tests have been added \n",
      "\n",
      "changes are very good , the author has proposed design patterns they will be used in the implementation , along with that method to calculate score has been explained elaborately \n",
      "\n",
      "the design appears to be sound , but i think more details are needed . \n",
      "\n",
      "1. the plan is sound and seems to be clearly explained bullet points \n",
      "\n",
      "the principles used to solving the problem are sound \n",
      "\n",
      "yes , the code is well written and follows the conventions of ruby design principles . \n",
      "\n",
      "code written following the coding standards \n",
      "\n"
     ]
    }
   ],
   "source": [
    "##################### Convert words back to check conversion \n",
    "for i in range(num_data):\n",
    "    comment = df['comments'][i]\n",
    "    length = len(comment)\n",
    "    count = 0;\n",
    "    text = []\n",
    "    for num in comment:\n",
    "        text.append(ix_to_word[num])\n",
    "        count += 1\n",
    "    text = (' '.join(text))\n",
    "    print(text,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[78, 22, 50, 15, 63, 75, 4, 52, 47, 58, 42, 79, 61, 85, 49, 83, 22, 15, 71, 17, 67, 46, 78, 30, 5]\n",
      "[78, 44, 13, 89, 9, 32, 78, 35, 88, 90, 5, 80, 91, 56, 86, 29, 46, 78, 30, 84, 5]\n",
      "[11, 4, 78, 96, 34, 78, 38, 88, 90, 4, 81, 41, 53, 50, 92, 27, 59, 81, 41, 87, 12, 93, 7, 5]\n",
      "[56, 55, 76, 41, 18, 8]\n",
      "[22, 15, 88, 39, 4, 78, 16, 40, 66, 27, 60, 81, 94, 17, 87, 46, 78, 45, 4, 10, 95, 77, 51, 85, 21, 69, 40, 18, 33, 31]\n",
      "[78, 27, 14, 85, 17, 73, 4, 20, 43, 82, 52, 28, 15, 54, 5]\n",
      "[6, 78, 61, 48, 73, 13, 70, 85, 17, 23, 33, 19, 62]\n",
      "[78, 64, 87, 85, 72, 78, 65, 15, 73]\n"
     ]
    }
   ],
   "source": [
    "############# Initialize training data\n",
    "train_data = df['comments'].head(8)\n",
    "for i in range(8):\n",
    "    print(train_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Initialize testing data **(requires full dataset conversion for tail() to work)**\n",
    "# print(df['comments'].tail(2))\n",
    "# test_data = df['comments'].tail(2)\n",
    "test_data = df['comments'].head(4) #### Remove later (PLACEHOLDER STATEMENT)\n",
    "# for i in range(2):\n",
    "#     print(test_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Function enabling conversion of indexed number sentence into word sentence\n",
    "def decode_review(text):\n",
    "#     return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
    "    return ' '.join([ix_to_word[i] for i in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the changes mentioned are pretty straightforward , more information on how their plan to make those changes are should be provided in the document .\n"
     ]
    }
   ],
   "source": [
    "print(decode_review(train_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 30\n",
      "Test: 30\n"
     ]
    }
   ],
   "source": [
    "############# Find the maximum sentence length to use for padding training data\n",
    "maxlength = 0\n",
    "for array in train_data:\n",
    "    maxlength = max(len(array),maxlength)\n",
    "print(\"Train:\", maxlength)\n",
    "for array in test_data:\n",
    "    maxlength = max(len(array),maxlength)\n",
    "print(\"Test:\", maxlength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Initialize training labels\n",
    "train_labels = df['is_prompt_exists'].head8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Pad the words to equalize array length\n",
    "train_data = keras.preprocessing.sequence.pad_sequences(train_data,\n",
    "                                                       value=word_to_ix[\"<PAD>\"],\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=maxlength)\n",
    "\n",
    "test_data = keras.preprocessing.sequence.pad_sequences(test_data,\n",
    "                                                      value=word_to_ix[\"<PAD>\"],\n",
    "                                                      padding='post',\n",
    "                                                      maxlen=maxlength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 30)"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data[0]), len(train_data[1]) # Check the new length of some train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[78 22 50 15 63 75  4 52 47 58 42 79 61 85 49 83 22 15 71 17 67 46 78 30\n",
      "  5  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0]) # Check new padded number sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 16)          116352    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 116,641\n",
      "Trainable params: 116,641\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "############# Creates the model\n",
    "vocab_size = len(df) # suggestions_data_balanced.csv dataset length becomes input shape: 7272\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, 16))\n",
    "model.add(keras.layers.GlobalAveragePooling1D())\n",
    "model.add(keras.layers.Dense(16, activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Configured model with optimizer and loss function\n",
    "model.compile(optimizer = tf.train.AdamOptimizer(),\n",
    "             loss = 'binary_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-436-4ea44d89e8c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpartial_x_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mpartial_y_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_labels' is not defined"
     ]
    }
   ],
   "source": [
    "########### Initiate validation sets\n",
    "# Note -> vocab_size = len(df)\n",
    "x_val = train_data[:vocab_size]\n",
    "partial_x_train = train_data[vocab_size:]\n",
    "\n",
    "y_val = train_labels[:vocab_size]\n",
    "partial_y_train = train_labels[vocab_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
